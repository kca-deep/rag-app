{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f385e04b",
   "metadata": {},
   "source": [
    "# 공공부문 AI 역량평가 문제 풀이 노트북"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 라이브러리 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 텍스트 분석용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 통계 분석용\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cf3d3",
   "metadata": {},
   "source": [
    "## 3세트: 공공일자리 참여자 분석 (공공일자리_참여자_정보.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d199a41",
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 불러오기 (인코딩 및 오류 처리 추가)\nimport os\n\ntry:\n    # 파일 경로 확인\n    file_path = \"공공일자리_참여자_정보.csv\"\n    if os.path.exists(file_path):\n        df_job = pd.read_csv(file_path, encoding='utf-8')\n    else:\n        # 샘플 데이터 생성 (실제 파일이 없는 경우)\n        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n        print(\"샘플 데이터를 생성합니다...\")\n        \n        np.random.seed(42)\n        sample_data = {\n            '참여자_연령대': np.random.choice(['20대', '30대', '40대', '50대', '60대'], 1000),\n            '참여자_성별': np.random.choice(['남성', '여성'], 1000),\n            '교육_수준': np.random.choice(['고졸', '대졸', '대학원'], 1000),\n            '참여_기간': np.random.randint(1, 12, 1000),\n            '취업_연계_여부': np.random.choice([0, 1], 1000, p=[0.6, 0.4])\n        }\n        df_job = pd.DataFrame(sample_data)\n    \n    print(f\"데이터 형태: {df_job.shape}\")\n    print(\"\\n컬럼 정보:\")\n    print(df_job.info())\n    print(\"\\n데이터 미리보기:\")\n    print(df_job.head())\n    \nexcept Exception as e:\n    print(f\"데이터 로딩 중 오류 발생: {e}\")\n    df_job = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f92544",
   "metadata": {},
   "outputs": [],
   "source": "# 데이터 전처리 및 모델링 (오류 처리 추가)\nif df_job is not None and not df_job.empty:\n    try:\n        # 결측값 확인 및 처리\n        print(\"결측값 확인:\")\n        print(df_job.isnull().sum())\n        \n        # 결측값이 있다면 처리\n        df_job = df_job.dropna()\n        \n        # 원-핫 인코딩 (숫자형 컬럼 제외)\n        categorical_columns = df_job.select_dtypes(include=['object']).columns\n        categorical_columns = [col for col in categorical_columns if col != '취업_연계_여부']\n        \n        if len(categorical_columns) > 0:\n            df_encoded = pd.get_dummies(df_job, columns=categorical_columns, drop_first=True)\n        else:\n            df_encoded = df_job.copy()\n        \n        # 특성과 타겟 분리\n        if '취업_연계_여부' in df_encoded.columns:\n            X = df_encoded.drop('취업_연계_여부', axis=1)\n            y = df_encoded['취업_연계_여부']\n        else:\n            print(\"타겟 변수 '취업_연계_여부'를 찾을 수 없습니다.\")\n            raise ValueError(\"타겟 변수가 없습니다.\")\n        \n        print(f\"특성 개수: {X.shape[1]}\")\n        print(f\"샘플 개수: {X.shape[0]}\")\n        \n        # train-test split\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n        \n        # 로지스틱 회귀\n        model = LogisticRegression(max_iter=1000, random_state=42)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        # 성능 지표\n        print(\"\\\\n=== 모델 성능 지표 ===\")\n        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n        print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n        print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n        print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n        \n        # 혼동 행렬 시각화\n        plt.figure(figsize=(6, 4))\n        cm = confusion_matrix(y_test, y_pred)\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['미연계', '연계'])\n        disp.plot(cmap='Blues')\n        plt.title('취업 연계 예측 혼동 행렬')\n        plt.tight_layout()\n        plt.show()\n        \n        # 계수 해석 (상위 10개)\n        print(\"\\\\n=== 특성 중요도 (상위 10개) ===\")\n        if len(X.columns) > 0:\n            coeffs = pd.Series(model.coef_[0], index=X.columns)\n            coeffs_sorted = coeffs.sort_values(ascending=False)\n            print(coeffs_sorted.head(10))\n            \n            # 계수 시각화\n            plt.figure(figsize=(10, 6))\n            coeffs_sorted.head(10).plot(kind='barh')\n            plt.title('특성별 중요도 (로지스틱 회귀 계수)')\n            plt.xlabel('계수 값')\n            plt.tight_layout()\n            plt.show()\n    \n    except Exception as e:\n        print(f\"모델링 중 오류 발생: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"데이터가 로드되지 않았거나 비어있습니다.\")"
  },
  {
   "cell_type": "markdown",
   "id": "fd19c5a4",
   "metadata": {},
   "source": [
    "## 4세트: 정책토론 게시글 분석 (정책토론_게시글.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0711c",
   "metadata": {},
   "outputs": [],
   "source": "# 정책토론 게시글 데이터 로딩 (오류 처리 추가)\ntry:\n    file_path = \"정책토론_게시글.csv\"\n    if os.path.exists(file_path):\n        df_forum = pd.read_csv(file_path, encoding='utf-8')\n    else:\n        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n        print(\"샘플 데이터를 생성합니다...\")\n        \n        # 샘플 텍스트 데이터 생성\n        sample_topics = [\n            \"교통 정책에 대한 의견입니다. 대중교통 확충이 필요합니다.\",\n            \"교육 정책 개선 방안을 제안합니다. 학생 중심 교육이 중요합니다.\",\n            \"환경 보호를 위한 정책이 시급합니다. 재생에너지 확대가 필요합니다.\",\n            \"복지 정책 확대를 통해 사회적 약자를 보호해야 합니다.\",\n            \"경제 활성화를 위한 중소기업 지원 정책이 필요합니다.\",\n            \"의료 서비스 개선을 통해 국민 건강을 증진시켜야 합니다.\",\n            \"주택 정책 개선으로 서민 주거 안정을 도모해야 합니다.\",\n            \"문화 예술 지원 정책을 통해 문화 발전을 도모합시다.\",\n        ]\n        \n        np.random.seed(42)\n        sample_data = {\n            '제목': [f\"정책 토론 {i+1}\" for i in range(200)],\n            '내용': np.random.choice(sample_topics, 200),\n            '작성일': pd.date_range('2023-01-01', periods=200, freq='D')\n        }\n        df_forum = pd.DataFrame(sample_data)\n    \n    print(f\"데이터 형태: {df_forum.shape}\")\n    print(\"\\\\n컬럼 정보:\")\n    print(df_forum.info())\n    print(\"\\\\n데이터 미리보기:\")\n    print(df_forum.head())\n    \nexcept Exception as e:\n    print(f\"데이터 로딩 중 오류 발생: {e}\")\n    df_forum = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e05e28",
   "metadata": {},
   "outputs": [],
   "source": "# 텍스트 분석 및 토픽 모델링 (오류 처리 추가)\nif df_forum is not None and not df_forum.empty:\n    try:\n        # 텍스트 데이터 전처리\n        if '내용' in df_forum.columns:\n            # 결측값 제거\n            df_forum_clean = df_forum.dropna(subset=['내용'])\n            texts = df_forum_clean['내용'].astype(str)\n            \n            print(f\"분석할 텍스트 개수: {len(texts)}\")\n            \n            # 텍스트 벡터화\n            vectorizer = CountVectorizer(\n                max_df=0.95, \n                min_df=2, \n                stop_words=None,\n                max_features=1000  # 최대 특성 수 제한\n            )\n            \n            try:\n                dtm = vectorizer.fit_transform(texts)\n                print(f\"벡터화 완료: {dtm.shape}\")\n                \n                # LDA 토픽 모델링\n                n_topics = min(5, dtm.shape[0] // 10)  # 데이터 크기에 따라 토픽 수 조정\n                if n_topics < 2:\n                    n_topics = 2\n                    \n                lda = LatentDirichletAllocation(\n                    n_components=n_topics, \n                    random_state=42,\n                    max_iter=10  # 반복 횟수 제한\n                )\n                lda.fit(dtm)\n                \n                # 토픽별 핵심 단어 추출\n                n_top_words = 10\n                feature_names = vectorizer.get_feature_names_out()\n                \n                print(\"\\\\n=== 토픽별 핵심 단어 ===\")\n                for topic_idx, topic in enumerate(lda.components_):\n                    top_indices = topic.argsort()[:-n_top_words - 1:-1]\n                    top_features = [feature_names[i] for i in top_indices]\n                    print(f\"토픽 {topic_idx+1}: {', '.join(top_features)}\")\n                \n                # 토픽 분포 시각화\n                plt.figure(figsize=(12, 6))\n                \n                # 문서별 토픽 분포\n                doc_topic_probs = lda.transform(dtm)\n                topic_avg = doc_topic_probs.mean(axis=0)\n                \n                plt.subplot(1, 2, 1)\n                plt.bar(range(1, n_topics+1), topic_avg)\n                plt.title('토픽별 평균 확률')\n                plt.xlabel('토픽 번호')\n                plt.ylabel('평균 확률')\n                \n                # 단어별 토픽 분포 (상위 토픽만)\n                plt.subplot(1, 2, 2)\n                top_topic_idx = np.argmax(topic_avg)\n                top_topic = lda.components_[top_topic_idx]\n                top_word_indices = top_topic.argsort()[-10:][::-1]\n                top_words = [feature_names[i] for i in top_word_indices]\n                top_probs = top_topic[top_word_indices]\n                \n                plt.barh(range(len(top_words)), top_probs)\n                plt.yticks(range(len(top_words)), top_words)\n                plt.title(f'토픽 {top_topic_idx+1}의 주요 단어')\n                plt.xlabel('확률')\n                \n                plt.tight_layout()\n                plt.show()\n                \n            except Exception as vec_error:\n                print(f\"벡터화 또는 토픽 모델링 오류: {vec_error}\")\n                print(\"단순 단어 빈도 분석을 수행합니다...\")\n                \n                # 대체: 단순 단어 빈도 분석\n                all_text = ' '.join(texts)\n                words = all_text.split()\n                word_freq = pd.Series(words).value_counts().head(20)\n                \n                plt.figure(figsize=(10, 6))\n                word_freq.plot(kind='barh')\n                plt.title('상위 20개 단어 빈도')\n                plt.xlabel('빈도')\n                plt.tight_layout()\n                plt.show()\n                \n                print(\"\\\\n상위 20개 단어:\")\n                print(word_freq)\n        else:\n            print(\"'내용' 컬럼을 찾을 수 없습니다.\")\n            print(f\"사용 가능한 컬럼: {list(df_forum.columns)}\")\n            \n    except Exception as e:\n        print(f\"텍스트 분석 중 오류 발생: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"포럼 데이터가 로드되지 않았거나 비어있습니다.\")"
  },
  {
   "cell_type": "markdown",
   "id": "f670de1e",
   "metadata": {},
   "source": [
    "## 5세트: 농산물 가격 분석 (주요농산물_일별_가격동향.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b033b8",
   "metadata": {},
   "outputs": [],
   "source": "# 농산물 가격 데이터 로딩 (오류 처리 추가)\ntry:\n    file_path = \"주요농산물_일별_가격동향.csv\"\n    if os.path.exists(file_path):\n        df_crop = pd.read_csv(file_path, encoding='utf-8')\n    else:\n        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n        print(\"샘플 데이터를 생성합니다...\")\n        \n        # 샘플 농산물 가격 데이터 생성\n        np.random.seed(42)\n        dates = pd.date_range('2023-01-01', periods=365, freq='D')\n        \n        sample_data = []\n        for date in dates:\n            # 배추 가격 (계절성 반영)\n            base_cabbage = 2000 + 500 * np.sin(2 * np.pi * date.dayofyear / 365)\n            cabbage_price = base_cabbage + np.random.normal(0, 200)\n            \n            # 사과 가격 (다른 패턴)\n            base_apple = 3000 + 300 * np.cos(2 * np.pi * date.dayofyear / 365)\n            apple_price = base_apple + np.random.normal(0, 150)\n            \n            sample_data.append({\n                '날짜': date.strftime('%Y-%m-%d'),\n                '품목': '배추',\n                '평균가격': max(cabbage_price, 500)  # 최소 가격 보장\n            })\n            sample_data.append({\n                '날짜': date.strftime('%Y-%m-%d'),\n                '품목': '사과',\n                '평균가격': max(apple_price, 1000)  # 최소 가격 보장\n            })\n        \n        df_crop = pd.DataFrame(sample_data)\n    \n    print(f\"데이터 형태: {df_crop.shape}\")\n    print(\"\\\\n컬럼 정보:\")\n    print(df_crop.info())\n    print(\"\\\\n데이터 미리보기:\")\n    print(df_crop.head())\n    print(\"\\\\n품목별 데이터 개수:\")\n    if '품목' in df_crop.columns:\n        print(df_crop['품목'].value_counts())\n    \nexcept Exception as e:\n    print(f\"데이터 로딩 중 오류 발생: {e}\")\n    df_crop = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00289d51",
   "metadata": {},
   "outputs": [],
   "source": "# 농산물 가격 상관관계 분석 (오류 처리 추가)\nif df_crop is not None and not df_crop.empty:\n    try:\n        # 필요한 품목이 있는지 확인\n        available_items = df_crop['품목'].unique() if '품목' in df_crop.columns else []\n        target_items = ['배추', '사과']\n        \n        print(f\"사용 가능한 품목: {available_items}\")\n        print(f\"분석 대상 품목: {target_items}\")\n        \n        # 대상 품목이 있는지 확인\n        existing_items = [item for item in target_items if item in available_items]\n        \n        if len(existing_items) >= 2:\n            # 배추, 사과 데이터 필터링 및 피벗\n            df_filtered = df_crop[df_crop['품목'].isin(existing_items)].copy()\n            \n            # 날짜 컬럼 처리\n            if '날짜' in df_filtered.columns:\n                df_filtered['날짜'] = pd.to_datetime(df_filtered['날짜'])\n                df_filtered = df_filtered.sort_values('날짜')\n            \n            # 피벗 테이블 생성\n            pivot = df_filtered.pivot(index='날짜', columns='품목', values='평균가격')\n            print(f\"\\\\n피벗 테이블 형태: {pivot.shape}\")\n            print(pivot.head())\n            \n            # 결측값 확인 및 처리\n            print(\"\\\\n결측값 개수:\")\n            print(pivot.isnull().sum())\n            \n            if len(existing_items) == 2:\n                item1, item2 = existing_items\n                \n                # 공통 날짜의 데이터만 사용\n                common_data = pivot.dropna()\n                print(f\"\\\\n공통 데이터 개수: {len(common_data)}\")\n                \n                if len(common_data) > 2:\n                    # 상관관계 계산\n                    corr, pval = pearsonr(common_data[item1], common_data[item2])\n                    print(f\"\\\\n=== {item1} vs {item2} 상관관계 ===\")\n                    print(f\"Correlation: {corr:.4f}\")\n                    print(f\"p-value: {pval:.4f}\")\n                    print(f\"유의성: {'유의함' if pval < 0.05 else '유의하지 않음'} (α=0.05)\")\n                    \n                    # 시각화\n                    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n                    \n                    # 시계열 그래프\n                    ax1_twin = ax1.twinx()\n                    \n                    line1 = ax1.plot(pivot.index, pivot[item1], label=f'{item1}', color='green', linewidth=2)\n                    line2 = ax1_twin.plot(pivot.index, pivot[item2], label=f'{item2}', color='red', linewidth=2)\n                    \n                    ax1.set_ylabel(f'{item1} 가격 (원)', color='green')\n                    ax1_twin.set_ylabel(f'{item2} 가격 (원)', color='red')\n                    ax1.set_title(f'{item1} vs {item2} 가격 추이 (상관계수: {corr:.3f})')\n                    ax1.grid(True, alpha=0.3)\n                    \n                    # 범례 설정\n                    lines = line1 + line2\n                    labels = [l.get_label() for l in lines]\n                    ax1.legend(lines, labels, loc='upper left')\n                    \n                    # 산점도\n                    ax2.scatter(common_data[item1], common_data[item2], alpha=0.6, s=30)\n                    ax2.set_xlabel(f'{item1} 가격 (원)')\n                    ax2.set_ylabel(f'{item2} 가격 (원)')\n                    ax2.set_title(f'{item1} vs {item2} 산점도')\n                    ax2.grid(True, alpha=0.3)\n                    \n                    # 추세선 추가\n                    z = np.polyfit(common_data[item1], common_data[item2], 1)\n                    p = np.poly1d(z)\n                    ax2.plot(common_data[item1], p(common_data[item1]), \n                            \"r--\", alpha=0.8, label=f'추세선 (기울기: {z[0]:.3f})')\n                    ax2.legend()\n                    \n                    plt.tight_layout()\n                    plt.show()\n                    \n                    # 추가 통계 정보\n                    print(f\"\\\\n=== 추가 통계 정보 ===\")\n                    print(f\"{item1} - 평균: {common_data[item1].mean():.2f}, 표준편차: {common_data[item1].std():.2f}\")\n                    print(f\"{item2} - 평균: {common_data[item2].mean():.2f}, 표준편차: {common_data[item2].std():.2f}\")\n                    \n                else:\n                    print(\"상관관계 분석을 위한 데이터가 충분하지 않습니다.\")\n            else:\n                print(\"분석을 위해 2개의 품목이 필요합니다.\")\n                \n        else:\n            print(f\"분석 대상 품목({target_items})을 찾을 수 없습니다.\")\n            print(f\"대신 사용 가능한 품목들의 가격 추이를 보여드립니다.\")\n            \n            # 사용 가능한 품목들의 가격 추이\n            if len(available_items) > 0:\n                plt.figure(figsize=(12, 6))\n                for item in available_items[:5]:  # 최대 5개 품목만\n                    item_data = df_crop[df_crop['품목'] == item]\n                    if '날짜' in item_data.columns:\n                        item_data['날짜'] = pd.to_datetime(item_data['날짜'])\n                        item_data = item_data.sort_values('날짜')\n                    plt.plot(item_data['날짜'], item_data['평균가격'], label=item, linewidth=2)\n                \n                plt.title('농산물 가격 추이')\n                plt.xlabel('날짜')\n                plt.ylabel('평균가격 (원)')\n                plt.legend()\n                plt.grid(True, alpha=0.3)\n                plt.xticks(rotation=45)\n                plt.tight_layout()\n                plt.show()\n    \n    except Exception as e:\n        print(f\"농산물 가격 분석 중 오류 발생: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"농산물 가격 데이터가 로드되지 않았거나 비어있습니다.\")"
  },
  {
   "cell_type": "markdown",
   "id": "7895e771",
   "metadata": {},
   "source": [
    "## 6세트: 감염병 신고 현황 분석 (감염병_신고현황.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d4a84",
   "metadata": {},
   "outputs": [],
   "source": "# 감염병 신고 데이터 로딩 (오류 처리 추가)\ntry:\n    file_path = \"감염병_신고현황.csv\"\n    if os.path.exists(file_path):\n        df_disease = pd.read_csv(file_path, encoding='utf-8')\n    else:\n        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n        print(\"샘플 데이터를 생성합니다...\")\n        \n        # 샘플 감염병 데이터 생성\n        np.random.seed(42)\n        \n        regions = ['서울특별시', '부산광역시', '대구광역시', '인천광역시', '광주광역시', \n                  '대전광역시', '울산광역시', '세종특별자치시', '경기도', '강원도',\n                  '충청북도', '충청남도', '전라북도', '전라남도', '경상북도', '경상남도', '제주특별자치도']\n        \n        diseases = ['인플루엔자', '코로나19', '결핵', '수두', '유행성이하선염', \n                   '백일해', '홍역', 'A형간염', '장티푸스', '세균성이질']\n        \n        age_groups = ['0-9세', '10-19세', '20-29세', '30-39세', '40-49세', \n                     '50-59세', '60-69세', '70-79세', '80세이상']\n        \n        sample_data = []\n        for _ in range(1000):\n            sample_data.append({\n                '지역': np.random.choice(regions),\n                '감염병명': np.random.choice(diseases),\n                '확진자_연령': np.random.choice(age_groups),\n                '확진자_수': np.random.randint(1, 100),\n                '신고일': pd.to_datetime('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))\n            })\n        \n        df_disease = pd.DataFrame(sample_data)\n    \n    print(f\"데이터 형태: {df_disease.shape}\")\n    print(\"\\\\n컬럼 정보:\")\n    print(df_disease.info())\n    print(\"\\\\n데이터 미리보기:\")\n    print(df_disease.head())\n    \n    if '지역' in df_disease.columns:\n        print(\"\\\\n지역별 데이터 개수:\")\n        print(df_disease['지역'].value_counts().head(10))\n    \n    if '감염병명' in df_disease.columns:\n        print(\"\\\\n감염병별 데이터 개수:\")\n        print(df_disease['감염병명'].value_counts())\n    \nexcept Exception as e:\n    print(f\"데이터 로딩 중 오류 발생: {e}\")\n    df_disease = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86babb",
   "metadata": {},
   "outputs": [],
   "source": "# 감염병 데이터 군집 분석 (오류 처리 추가)\nif df_disease is not None and not df_disease.empty:\n    try:\n        # 데이터 전처리\n        df_analysis = df_disease.copy()\n        \n        # 지역 단위 변환 (시/군 → 시/도)\n        if '지역' in df_analysis.columns:\n            # 광역시/도 단위로 변환\n            df_analysis['시도'] = df_analysis['지역'].str.extract(r'^([가-힣]+[시도]|[가-힣]+특별[시자치시]|[가-힣]+광역시)')\n            df_analysis['시도'] = df_analysis['시도'].fillna(df_analysis['지역'].str[:2])\n        else:\n            print(\"'지역' 컬럼을 찾을 수 없습니다.\")\n            df_analysis['시도'] = '기타'\n        \n        print(\"시도별 데이터 분포:\")\n        print(df_analysis['시도'].value_counts())\n        \n        # 피처 선택 및 전처리\n        features_to_encode = []\n        if '시도' in df_analysis.columns:\n            features_to_encode.append('시도')\n        if '감염병명' in df_analysis.columns:\n            features_to_encode.append('감염병명')\n        if '확진자_연령' in df_analysis.columns:\n            features_to_encode.append('확진자_연령')\n        \n        if len(features_to_encode) == 0:\n            print(\"인코딩할 피처가 없습니다.\")\n            raise ValueError(\"분석할 수 있는 범주형 변수가 없습니다.\")\n        \n        print(f\"\\\\n인코딩할 피처: {features_to_encode}\")\n        \n        # 원-핫 인코딩\n        X = pd.get_dummies(df_analysis[features_to_encode], drop_first=True)\n        print(f\"인코딩 후 피처 수: {X.shape[1]}\")\n        print(f\"샘플 수: {X.shape[0]}\")\n        \n        if X.shape[1] == 0 or X.shape[0] < 4:\n            print(\"군집 분석을 위한 데이터가 충분하지 않습니다.\")\n            raise ValueError(\"데이터가 부족합니다.\")\n        \n        # 최적 군집 수 결정 (엘보우 방법)\n        max_clusters = min(10, X.shape[0] // 2)\n        inertias = []\n        cluster_range = range(2, max_clusters + 1)\n        \n        print(f\"\\\\n최적 군집 수 탐색 (2~{max_clusters}개)\")\n        for k in cluster_range:\n            kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n            kmeans_temp.fit(X)\n            inertias.append(kmeans_temp.inertia_)\n        \n        # 엘보우 방법 시각화\n        plt.figure(figsize=(15, 5))\n        \n        plt.subplot(1, 3, 1)\n        plt.plot(cluster_range, inertias, 'bo-')\n        plt.xlabel('군집 수')\n        plt.ylabel('Inertia')\n        plt.title('엘보우 방법을 이용한 최적 군집 수')\n        plt.grid(True, alpha=0.3)\n        \n        # 최적 군집 수 선택 (엘보우 포인트 추정)\n        if len(inertias) >= 3:\n            # 2차 차분을 이용한 엘보우 포인트 찾기\n            diffs = np.diff(inertias)\n            second_diffs = np.diff(diffs)\n            if len(second_diffs) > 0:\n                optimal_k = np.argmax(second_diffs) + 3  # +3은 인덱스 보정\n                optimal_k = min(optimal_k, max_clusters)\n            else:\n                optimal_k = 4\n        else:\n            optimal_k = 4\n        \n        print(f\"선택된 군집 수: {optimal_k}\")\n        \n        # K-means 군집화\n        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n        clusters = kmeans.fit_predict(X)\n        df_analysis['cluster'] = clusters\n        \n        print(f\"\\\\n군집별 데이터 분포:\")\n        print(pd.Series(clusters).value_counts().sort_index())\n        \n        # PCA 시각화 (2D)\n        if X.shape[1] >= 2:\n            pca = PCA(n_components=2)\n            X_pca = pca.fit_transform(X)\n            \n            explained_variance = pca.explained_variance_ratio_\n            print(f\"\\\\nPCA 설명 분산 비율: {explained_variance}\")\n            print(f\"총 설명 분산: {explained_variance.sum():.3f}\")\n            \n            plt.subplot(1, 3, 2)\n            scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='rainbow', alpha=0.7)\n            plt.colorbar(scatter)\n            plt.xlabel(f'PC1 ({explained_variance[0]:.2%})')\n            plt.ylabel(f'PC2 ({explained_variance[1]:.2%})')\n            plt.title('감염병 패턴 군집화 (PCA 2D)')\n            plt.grid(True, alpha=0.3)\n            \n            # 군집 중심점 표시\n            centers_pca = pca.transform(kmeans.cluster_centers_)\n            plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n                       c='black', marker='x', s=200, linewidths=3, label='중심점')\n            plt.legend()\n        \n        # 군집별 특성 분석\n        plt.subplot(1, 3, 3)\n        cluster_counts = pd.Series(clusters).value_counts().sort_index()\n        plt.bar(cluster_counts.index, cluster_counts.values)\n        plt.xlabel('군집 번호')\n        plt.ylabel('데이터 개수')\n        plt.title('군집별 데이터 분포')\n        plt.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # 군집별 상세 분석\n        print(\"\\\\n=== 군집별 특성 분석 ===\")\n        for cluster_id in sorted(df_analysis['cluster'].unique()):\n            cluster_data = df_analysis[df_analysis['cluster'] == cluster_id]\n            print(f\"\\\\n군집 {cluster_id} (총 {len(cluster_data)}개)\")\n            \n            # 주요 특성별 분포\n            for feature in features_to_encode:\n                if feature in cluster_data.columns:\n                    top_values = cluster_data[feature].value_counts().head(3)\n                    print(f\"  {feature}: {dict(top_values)}\")\n        \n        # 실루엣 점수 계산\n        from sklearn.metrics import silhouette_score\n        if len(set(clusters)) > 1 and len(set(clusters)) < len(clusters):\n            sil_score = silhouette_score(X, clusters)\n            print(f\"\\\\n실루엣 점수: {sil_score:.3f}\")\n            print(\"실루엣 점수 해석:\")\n            if sil_score > 0.7:\n                print(\"  - 매우 좋은 군집화\")\n            elif sil_score > 0.5:\n                print(\"  - 좋은 군집화\")\n            elif sil_score > 0.25:\n                print(\"  - 보통 군집화\")\n            else:\n                print(\"  - 개선이 필요한 군집화\")\n        \n    except Exception as e:\n        print(f\"군집 분석 중 오류 발생: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"감염병 데이터가 로드되지 않았거나 비어있습니다.\")"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}